{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Accelerate \u7279\u5f81 \u63d0\u4f9b\u6df7\u5408\u7cbe\u5ea6\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\uff08\u5355\u591aGPU\u3001\u591a\u673a\u591aGPU\u3001TPU\u7b49\u7b49\uff09\u7b49\u529f\u80fd\uff0c\u5728\u539f\u8bad\u7ec3\u4ee3\u7801\u7684\u57fa\u7840\u4e0a\u53ea\u9700\u8981\u4fee\u6539\u5c11\u91cf\u4ee3\u7801\uff1b \u63d0\u4f9b\u547d\u4ee4\u884c\u5de5\u5177\u53ef\u4ee5\u5feb\u901f\u6d4b\u8bd5\u672c\u5730\u73af\u5883 \u7528\u6cd5\u7b80\u5355\uff0c\u4ee3\u7801\u4fee\u6539\u91cf\u5c11 \u4f20\u7edf\u7684\u4f7f\u7528pytorch\u7684\u8bad\u7ec3\u4ee3\u7801\u4e00\u822c\u5982\u4e0b\uff1a my_model.to(device) for batch in my_training_dataloader: my_optimizer.zero_grad() inputs, targets = batch inputs = inputs.to(device) targets = targets.to(device) outputs = my_model(inputs) loss = my_loss_function(outputs, targets) loss.backward() my_optimizer.step() \u5982\u679c\u8981\u4f7f\u7528accelerate\u53ea\u9700\u505a\u5982\u4e0b\u4ee3\u7801\u7684\u4fee\u6539\u5c31\u53ef\u4ee5\u8f7b\u677e\u8fd0\u884c\u5230\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\uff1a + from accelerate import Accelerator + accelerator = Accelerator() # Use the device given by the `accelerator` object. + device = accelerator.device my_model.to(device) # Pass every important object (model, optimizer, dataloader) to `accelerator.prepare` + my_model, my_optimizer, my_training_dataloader = accelerate.prepare( + my_model, my_optimizer, my_training_dataloader + ) for batch in my_training_dataloader: my_optimizer.zero_grad() inputs, targets = batch inputs = inputs.to(device) targets = targets.to(device) outputs = my_model(inputs) loss = my_loss_function(outputs, targets) # Just a small change for the backward instruction - loss.backward() + accelerate.backward(loss) my_optimizer.step() \u4e0a\u8ff0\u65b9\u5f0f\u8fd8\u9700\u8981\u81ea\u5df1\u901a\u8fc7\u51fd\u6570 to(device) \u6307\u5b9a\u8bbe\u5907\uff0c\u66f4\u63a8\u8350\u7684\u7528\u6cd5\u662f\u7531 Accelerator \u81ea\u52a8\u7ba1\u7406\u8bbe\u5907\u4fe1\u606f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a + from accelerate import Accelerator + accelerator = Accelerator() - my_model.to(device) # Pass every important object (model, optimizer, dataloader) to `accelerator.prepare` + my_model, my_optimizer, my_training_dataloader = accelerate.prepare( + my_model, my_optimizer, my_training_dataloader + ) for batch in my_training_dataloader: my_optimizer.zero_grad() inputs, targets = batch - inputs = inputs.to(device) - targets = targets.to(device) outputs = my_model(inputs) loss = my_loss_function(outputs, targets) # Just a small change for the backward instruction - loss.backward() + accelerate.backward(loss) my_optimizer.step() \u652f\u6301\u7684\u73af\u5883 CPU \u5355GPU \u591aGPU\u5355\u8282\u70b9 \u591aGPU\u591a\u8282\u70b9 TPU FP16 with native AMP DeepSpeed \u76ee\u5f55 Get Started Quick tour \u4e3b\u8981\u7528\u6cd5 \u5206\u5e03\u5f0f\u8bc4\u4f30 \u8fd0\u884c\u5206\u5e03\u5f0f\u811a\u672c \u4f7f\u7528notebook\u8fd0\u884c \u5728TPU\u4e0a\u8bad\u7ec3 \u5176\u4ed6\u6ce8\u610f\u70b9\u8bf4\u660e \u5185\u90e8\u539f\u7406\u548c\u673a\u5236 Installation API Reference Accelerate Notebook Launcher Kwargs Handlers Internals","title":"Accelerate"},{"location":"#accelerate","text":"","title":"Accelerate"},{"location":"#_1","text":"\u63d0\u4f9b\u6df7\u5408\u7cbe\u5ea6\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\uff08\u5355\u591aGPU\u3001\u591a\u673a\u591aGPU\u3001TPU\u7b49\u7b49\uff09\u7b49\u529f\u80fd\uff0c\u5728\u539f\u8bad\u7ec3\u4ee3\u7801\u7684\u57fa\u7840\u4e0a\u53ea\u9700\u8981\u4fee\u6539\u5c11\u91cf\u4ee3\u7801\uff1b \u63d0\u4f9b\u547d\u4ee4\u884c\u5de5\u5177\u53ef\u4ee5\u5feb\u901f\u6d4b\u8bd5\u672c\u5730\u73af\u5883","title":"\u7279\u5f81"},{"location":"#_2","text":"\u4f20\u7edf\u7684\u4f7f\u7528pytorch\u7684\u8bad\u7ec3\u4ee3\u7801\u4e00\u822c\u5982\u4e0b\uff1a my_model.to(device) for batch in my_training_dataloader: my_optimizer.zero_grad() inputs, targets = batch inputs = inputs.to(device) targets = targets.to(device) outputs = my_model(inputs) loss = my_loss_function(outputs, targets) loss.backward() my_optimizer.step() \u5982\u679c\u8981\u4f7f\u7528accelerate\u53ea\u9700\u505a\u5982\u4e0b\u4ee3\u7801\u7684\u4fee\u6539\u5c31\u53ef\u4ee5\u8f7b\u677e\u8fd0\u884c\u5230\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\uff1a + from accelerate import Accelerator + accelerator = Accelerator() # Use the device given by the `accelerator` object. + device = accelerator.device my_model.to(device) # Pass every important object (model, optimizer, dataloader) to `accelerator.prepare` + my_model, my_optimizer, my_training_dataloader = accelerate.prepare( + my_model, my_optimizer, my_training_dataloader + ) for batch in my_training_dataloader: my_optimizer.zero_grad() inputs, targets = batch inputs = inputs.to(device) targets = targets.to(device) outputs = my_model(inputs) loss = my_loss_function(outputs, targets) # Just a small change for the backward instruction - loss.backward() + accelerate.backward(loss) my_optimizer.step() \u4e0a\u8ff0\u65b9\u5f0f\u8fd8\u9700\u8981\u81ea\u5df1\u901a\u8fc7\u51fd\u6570 to(device) \u6307\u5b9a\u8bbe\u5907\uff0c\u66f4\u63a8\u8350\u7684\u7528\u6cd5\u662f\u7531 Accelerator \u81ea\u52a8\u7ba1\u7406\u8bbe\u5907\u4fe1\u606f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a + from accelerate import Accelerator + accelerator = Accelerator() - my_model.to(device) # Pass every important object (model, optimizer, dataloader) to `accelerator.prepare` + my_model, my_optimizer, my_training_dataloader = accelerate.prepare( + my_model, my_optimizer, my_training_dataloader + ) for batch in my_training_dataloader: my_optimizer.zero_grad() inputs, targets = batch - inputs = inputs.to(device) - targets = targets.to(device) outputs = my_model(inputs) loss = my_loss_function(outputs, targets) # Just a small change for the backward instruction - loss.backward() + accelerate.backward(loss) my_optimizer.step()","title":"\u7528\u6cd5\u7b80\u5355\uff0c\u4ee3\u7801\u4fee\u6539\u91cf\u5c11"},{"location":"#_3","text":"CPU \u5355GPU \u591aGPU\u5355\u8282\u70b9 \u591aGPU\u591a\u8282\u70b9 TPU FP16 with native AMP DeepSpeed","title":"\u652f\u6301\u7684\u73af\u5883"},{"location":"#_4","text":"","title":"\u76ee\u5f55"},{"location":"#get-started","text":"Quick tour \u4e3b\u8981\u7528\u6cd5 \u5206\u5e03\u5f0f\u8bc4\u4f30 \u8fd0\u884c\u5206\u5e03\u5f0f\u811a\u672c \u4f7f\u7528notebook\u8fd0\u884c \u5728TPU\u4e0a\u8bad\u7ec3 \u5176\u4ed6\u6ce8\u610f\u70b9\u8bf4\u660e \u5185\u90e8\u539f\u7406\u548c\u673a\u5236 Installation","title":"Get Started"},{"location":"#api-reference","text":"Accelerate Notebook Launcher Kwargs Handlers Internals","title":"API Reference"},{"location":"Installation/","text":"Installation \ud83e\udd17 Accelerate is tested on Python 3.6+, and PyTorch 1.6.0+. You should install \ud83e\udd17 Accelerate in a virtual environment. If you\u2019re unfamiliar with Python virtual environments, check out the user guide. Create a virtual environment with the version of Python you\u2019re going to use and activate it. Now, if you want to use \ud83e\udd17 Accelerate, you can install it with pip. Installation with pip First you need to install PyTorch. Please refer to the PyTorch installation page regarding the specific install command for your platform. When PyTorch has been installed, \ud83e\udd17 Accelerate can be installed using pip as follows: pip install accelerate Alternatively, for CPU-support only, you can install \ud83e\udd17 Accelerate and PyTorch in one line with: pip install accelerate[torch] To check \ud83e\udd17 Accelerate is properly installed, run the following command: python -c \"TODO write\" Installing from source Here is how to quickly install accelerate from source: pip install git+https://github.com/huggingface/accelerate Note that this will install not the latest released version, but the bleeding edge master version, which you may want to use in case a bug has been fixed since the last official release and a new release hasn\u2019t been yet rolled out. While we strive to keep master operational at all times, if you notice some issues, they usually get fixed within a few hours or a day and and you\u2019re more than welcome to help us detect any problems by opening an Issue and this way, things will get fixed even sooner. Again, you can run: python -c \"TODO write\" to check \ud83e\udd17 Accelerate is properly installed. Editable install If you want to constantly use the bleeding edge master version of the source code, or if you want to contribute to the library and need to test the changes in the code you\u2019re making, you will need an editable install. This is done by cloning the repository and installing with the following commands: git clone https://github.com/huggingface/accelerate.git cd transformers pip install -e . This command performs a magical link between the folder you cloned the repository to and your python library paths, and it\u2019ll look inside this folder in addition to the normal library-wide paths. So if normally your python packages get installed into: ~/anaconda3/envs/main/lib/python3.7/site-packages/ now this editable install will reside where you clone the folder to, e.g. ~/accelerate/ and python will search it too. Do note that you have to keep that accelerate folder around and not delete it to continue using the \ud83e\udd17 Accelerate library. Now, let\u2019s get to the real benefit of this installation approach. Say, you saw some new feature has been just committed into master. If you have already performed all the steps above, to update your accelerate repo to include all the latest commits, all you need to do is to cd into that cloned repository folder and update the clone to the latest version: cd ~/accelerate/ git pull There is nothing else to do. Your python environment will find the bleeding edge version of \ud83e\udd17 Accelerate on the next run.","title":"Installation"},{"location":"Installation/#installation","text":"\ud83e\udd17 Accelerate is tested on Python 3.6+, and PyTorch 1.6.0+. You should install \ud83e\udd17 Accelerate in a virtual environment. If you\u2019re unfamiliar with Python virtual environments, check out the user guide. Create a virtual environment with the version of Python you\u2019re going to use and activate it. Now, if you want to use \ud83e\udd17 Accelerate, you can install it with pip.","title":"Installation"},{"location":"Installation/#installation-with-pip","text":"First you need to install PyTorch. Please refer to the PyTorch installation page regarding the specific install command for your platform. When PyTorch has been installed, \ud83e\udd17 Accelerate can be installed using pip as follows: pip install accelerate Alternatively, for CPU-support only, you can install \ud83e\udd17 Accelerate and PyTorch in one line with: pip install accelerate[torch] To check \ud83e\udd17 Accelerate is properly installed, run the following command: python -c \"TODO write\"","title":"Installation with pip"},{"location":"Installation/#installing-from-source","text":"Here is how to quickly install accelerate from source: pip install git+https://github.com/huggingface/accelerate Note that this will install not the latest released version, but the bleeding edge master version, which you may want to use in case a bug has been fixed since the last official release and a new release hasn\u2019t been yet rolled out. While we strive to keep master operational at all times, if you notice some issues, they usually get fixed within a few hours or a day and and you\u2019re more than welcome to help us detect any problems by opening an Issue and this way, things will get fixed even sooner. Again, you can run: python -c \"TODO write\" to check \ud83e\udd17 Accelerate is properly installed.","title":"Installing from source"},{"location":"Installation/#editable-install","text":"If you want to constantly use the bleeding edge master version of the source code, or if you want to contribute to the library and need to test the changes in the code you\u2019re making, you will need an editable install. This is done by cloning the repository and installing with the following commands: git clone https://github.com/huggingface/accelerate.git cd transformers pip install -e . This command performs a magical link between the folder you cloned the repository to and your python library paths, and it\u2019ll look inside this folder in addition to the normal library-wide paths. So if normally your python packages get installed into: ~/anaconda3/envs/main/lib/python3.7/site-packages/ now this editable install will reside where you clone the folder to, e.g. ~/accelerate/ and python will search it too. Do note that you have to keep that accelerate folder around and not delete it to continue using the \ud83e\udd17 Accelerate library. Now, let\u2019s get to the real benefit of this installation approach. Say, you saw some new feature has been just committed into master. If you have already performed all the steps above, to update your accelerate repo to include all the latest commits, all you need to do is to cd into that cloned repository folder and update the clone to the latest version: cd ~/accelerate/ git pull There is nothing else to do. Your python environment will find the bleeding edge version of \ud83e\udd17 Accelerate on the next run.","title":"Editable install"},{"location":"accelerator/","text":"","title":"Accelerator"},{"location":"internals/","text":"","title":"Internals"},{"location":"kwargs_handlers/","text":"","title":"Kwargs Handlers"},{"location":"notebook_launcher/","text":"","title":"Notebook Launcher"},{"location":"quick_tour/","text":"Quick tour \u4e3b\u8981\u7528\u6cd5 \u5728\u4e4b\u524d\u7684\u6587\u6863 Accelerate \u4e2d\u5df2\u7ecf\u770b\u5230\u8fc7\u8981\u60f3\u4f7f\u7528Accelerate\u9700\u8981\u505a\u7684\u4ee3\u7801\u4fee\u6539\u5982\u4e0b\u6240\u793a\uff1a + from accelerate import Accelerator + accelerator = Accelerator() - my_model.to(device) # Pass every important object (model, optimizer, dataloader) to `accelerator.prepare` + my_model, my_optimizer, my_training_dataloader = accelerate.prepare( + my_model, my_optimizer, my_training_dataloader + ) for batch in my_training_dataloader: my_optimizer.zero_grad() inputs, targets = batch - inputs = inputs.to(device) - targets = targets.to(device) outputs = my_model(inputs) loss = my_loss_function(outputs, targets) # Just a small change for the backward instruction - loss.backward() + accelerate.backward(loss) my_optimizer.step() \u4e0a\u8ff0\u4ee3\u7801\u7684\u4fee\u6539\u53ef\u4ee5\u5206\u4e3a\u56db\u4e2a\u6b65\u9aa4\u8fdb\u884c\uff1a \u6b65\u9aa4\u4e00 \uff1aimport \u5e76\u5b9e\u4f8b\u5316 Accelerator \u5bf9\u8c61\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a from accelerate import Accelerator accelerator = Accelerator() \u4e0a\u8ff0\u8fd9\u4e2a\u521d\u59cb\u5316\u4ee3\u7801\u4f1a\u81ea\u52a8\u68c0\u6d4b\u8bbe\u5907\u548c\u73af\u5883\u4fe1\u606f\uff08\u5355CPU\u3001\u591aCPU\u3001\u5355GPU\u3001\u591aGPU\u3001TPU\u7b49\u7b49\u90fd\u4f1a\u81ea\u52a8\u68c0\u6d4b\u5230\uff09\uff0c\u5f00\u53d1\u8005\u5b8c\u5168\u4e0d\u9700\u8981\u5173\u5fc3\u8bbe\u5907\u548c\u73af\u5883\u4fe1\u606f\u3002 \u9700\u8981\u6ce8\u610f\u7684\u4e00\u70b9\u662f\uff1a\u4e0a\u8ff0\u521d\u59cb\u5316\u4ee3\u7801\u5fc5\u987b\u8981\u5728\u6240\u6709\u8bad\u7ec3\u6709\u5173\u7684\u4ee3\u7801\u4e4b\u524d\u6267\u884c\uff1b \u6b65\u9aa4\u4e8c \uff1a\u5220\u9664\u539f\u4ee3\u7801\u4e2d\u7684 to(device) \u548c cuda() \uff0cAccelerator \u4f1a\u81ea\u52a8\u5c06model\u3001data\u7b49\u653e\u5230\u6b63\u786e\u7684\u8bbe\u5907\u4e0a\uff0c\u5f00\u53d1\u8005\u4e0d\u7528\u5173\u5fc3\u3002 \u5f53\u7136\uff0c\u5982\u679c\u662f\u9ad8\u7aef\u73a9\u5bb6\u60f3\u8981\u81ea\u5df1\u6307\u5b9a\u8bbe\u5907\u4fe1\u606f\u4e5f\u53ef\u4ee5\uff0c\u6b64\u65f6 to(device) \u4e2d\u7684 device \u9700\u8981\u662f accelerator.device \u3002 \u6700\u540e\uff0c\u5982\u679c\u5b8c\u5168\u4e0d\u60f3\u4f7f\u7528 Accelerator \u81ea\u52a8\u7ba1\u7406\u8bbe\u5907\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u5728\u521d\u59cb\u5316\u65f6\u4f20\u5165\u53c2\u6570 device_placement=False \uff0c\u4ee3\u7801\u5982\u4e0b\uff1a accelerator = Accelerator(device_placement=False) \u6b65\u9aa4\u4e09 \uff1a\u5c06\u6240\u6709\u548c\u8bad\u7ec3\u76f8\u5173\u7684\u5bf9\u8c61\uff08 optimizer , model , training dataloader \uff09\u4f20\u5165\u51fd\u6570 prepare() \u3002\u8be5\u6b65\u9aa4\u5e94\u8be5\u653e\u5728\u6240\u6709\u76f8\u5173\u5bf9\u8c61\u521d\u59cb\u5316\u4e4b\u540e\uff0c\u548c\u8bad\u7ec3\u5f00\u59cb\u4e4b\u524d\u3002\u4ee3\u7801\u5982\u4e0b\uff1a model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader) \u6ce8\u610f\uff1a train_dataloader \uff1a\u5f53\u505a\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u4ee5\u591aGPU\u4e3a\u4f8b\uff0c\u6240\u6709\u7684\u8bad\u7ec3\u6570\u636e\u4f1a\u5e73\u5747\u5206\u914d\u5230\u6bcf\u4e2aGPU\u4e0a\u3002\u5f53\u8bad\u7ec3\u6570\u636e\u9700\u8981 shuffle \u65f6\uff0c\u5176\u5b9e\u73b0\u539f\u7406\u662f\uff1a \u5c06\u968f\u673a\u72b6\u6001\uff08random state\uff09\u540c\u6b65\u5230\u6240\u6709\u7684GPU\uff0c\u4ee5\u6b64\u4fdd\u8bc1\u6bcf\u4e2aGPU\u4e0a\u8bad\u7ec3\u6570\u636e shuffle \u4e4b\u540e\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff1b \u6bcf\u4e2aGPU\u8bfb\u53d6 shuffle \u540e\u7684\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u540c\u7247\u6bb5\uff1b\u6bd4\u5982\u67092\u4e2aGPU\u65f6\uff0c\u7b2c\u4e00\u4e2aGPU\u8bfb\u53d6\u7b2c [1, 3, 5, 7, ...] \u6761\u6570\u636e\uff0c\u7b2c\u4e8c\u4e2aGPU\u8bfb\u53d6\u7b2c [0, 2, 4, 6, ...] \u6761\u6570\u636e\uff1b batch_size \uff1a\u5f53\u505a\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u4ee5\u591aGPU\u4e3a\u4f8b\uff1a\u5b9e\u9645\u7684batch_size = \u8f93\u5165\u7684batch_size * GPU\u6570\u91cf\u3002\u6bd4\u5982\u8f93\u5165batch_size\u4e3a16\uff0c\u5f53\u4f7f\u75284\u4e2aGPU\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u5b9e\u9645\u7684batch_size\u4e3a64\u3002 validation dataloader \uff1a\u4e0a\u9762\u4f20\u5165 prepare() \u7684\u53ea\u6709 train_dataloader \uff0c\u53e6\u5916 validation dataloader \u4e5f\u53ef\u4ee5\u4f20\u5165 prepare() \uff0c\u5177\u4f53\u7684\u4f7f\u7528\u65b9\u5f0f\u89c1\u4e0b\u9762\u7684 \u5206\u5e03\u5f0f\u8bc4\u4f30 \u90e8\u5206\u3002 \u6b65\u9aa4\u56db \uff1a\u5c06 loss.backward() \u66ff\u6362\u6210 accelerator.backward(loss) \u3002 \u5b8c\u6210\u4ee5\u4e0a\u56db\u4e2a\u6b65\u9aa4\u4e4b\u540e\u5c31\u53ef\u4ee5\u6b63\u5e38\u7684\u4f7f\u7528 Accelerator \u4e86\u3002 \u5206\u5e03\u5f0f\u8bc4\u4f30 \u5982\u679c\u8981\u505a\u5206\u5e03\u5f0f\u8bc4\u4f30\uff0c\u53ea\u9700\u8981\u5c06 validation_dataloader \u4e5f\u4f20\u5165 prepare() \uff0c\u4ee3\u7801\u5982\u4e0b\uff1a validation_dataloader = accelerator.prepare(validation_dataloader) \u548c train_dataloader \u7684\u5b9e\u73b0\u539f\u7406\u7c7b\u4f3c\uff0c validation_dataloader \u4e2d\u6240\u6709\u7684\u6570\u636e\u4f1a\u88ab\u5212\u5206\u5230\u591a\u4e2aGPU\u4e0a\uff0c\u6240\u4ee5\u5728\u6a21\u578b\u524d\u5411\u4f20\u64ad\u5b8c\u6210\u4e4b\u540e\u8981\u8c03\u7528\u51fd\u6570 gather() \u505a\u4e00\u4e0b\u805a\u5408\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a for inputs, targets in validation_dataloader: predictions = model(inputs) # Gather all predictions and targets all_predictions = accelerator.gather(predictions) all_targets = accelerator.gather(targets) # Example of use with a `Datasets.Metric` metric.add_batch(all_predictions, all_targets) \u6ce8\u610f\uff1a\u51fd\u6570 gather() \u8981\u6c42\u6240\u6709\u7684 tensor \u5177\u6709\u76f8\u540c\u7684\u7ef4\u5ea6\u3002\u6bd4\u5982\uff1a\u6bcf\u4e2aGPU\u4e0a\u7684\u6570\u636e\u5728 padding \u65f6\uff0cpadding \u7684\u6700\u5927\u957f\u5ea6\u8bbe\u7f6e\u4e3a\u5f53\u524d\u8fd9\u4e2a mini_batch=16 \u6761\u6570\u636e\u4e2d\u957f\u5ea6\u6700\u5927\u7684\u90a3\u6761\u6570\u636e\u7684\u957f\u5ea6\uff0c\u5c31\u4f1a\u5bfc\u81f4\u591a\u4e2aGPU\u4e4b\u95f4\u7684 tensor \u7684\u7ef4\u5ea6\u4e0d\u540c\uff0c\u6b64\u65f6\u51fd\u6570 gather() \u4e0d\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002 \u8fd0\u884c\u5206\u5e03\u5f0f\u811a\u672c \u6ca1\u6709\u770b \u4f7f\u7528notebook\u8fd0\u884c \u6ca1\u6709\u770b \u5728TPU\u4e0a\u8bad\u7ec3 \u6211\u54ea\u6709TPU \u5176\u4ed6\u6ce8\u610f\u70b9\u8bf4\u660e \u7a0b\u5e8f\u53ea\u5728\u4e00\u4e2a\u8fdb\u7a0b\u4e2d\u6267\u884c \u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u4ee5\u591aGPU\u4e3a\u4f8b\uff0c\u6211\u4eec\u7684\u7a0b\u5e8f\u4f1a\u540c\u65f6\u8fd0\u884c\u5728\u591a\u5f20\u663e\u5361\u4e0a\u3002\u90a3\u4e48\u6bd4\u5982\u6253\u5370\u65e5\u5fd7\u3001\u6253\u5370\u8fdb\u5ea6\u6761\u7b49\u529f\u80fd\uff0c\u53ea\u5e0c\u671b\u5176\u6267\u884c\u4e00\u6b21\u5c31\u884c\u4e86\uff0c\u5426\u5219\u5982\u679c\u67098\u5f20\u663e\u5361\uff0c\u540c\u4e00\u6761\u65e5\u5fd7\u6253\u53708\u6b21\u770b\u8d77\u6765\u5f88\u4e0d\u65b9\u4fbf\u3002 \u60f3\u8981\u5b9e\u73b0\u5e76\u53d1\u8bad\u7ec3\u65f6\u7a0b\u5e8f\u53ea\u6267\u884c\u4e00\u6b21\uff0c\u53ea\u9700\u5c06\u7a0b\u5e8f\u5305\u5230\u5982\u4e0b if \u5224\u65ad\u5185\u90e8\u5373\u53ef\uff1a if accelerator.is_local_main_process: # Is executed once per server \u5176\u4e2d\u6807\u5fd7 accelerator.is_local_main_process \u5c31\u8868\u793a\u662f\u5426\u5728\u4e3b\u8fdb\u7a0b\u4e2d\u3002\u5728\u8fd9\u4e2a if \u5224\u65ad\u5185\u90e8\u7684\u4ee3\u7801\uff0c\u5982\u679c\u4e0d\u662f\u5728\u4e3b\u8fdb\u7a0b\u4e2d\uff0c\u662f\u4e0d\u4f1a\u6267\u884c\u7684\u3002 \u50cf\u6253\u5370\u8fdb\u5ea6\u6761\u7684 tqdm \u5e93\uff0c\u53ef\u4ee5\u6309\u7167\u5982\u4e0b\u65b9\u5f0f\u4f7f\u7528\uff1a from tqdm.auto import tqdm progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process) \u7c7b\u4f3c\u7684\uff0c\u53ea\u6709\u5728\u4e3b\u8fdb\u7a0b\u4e2d\u624d\u4f1a\u6253\u5370\u8fdb\u5ea6\u6761\uff1b \u6807\u5fd7 accelerator.is_local_main_process \u4e2d\u7684 local \u662f\u5f53\u524d\u673a\u5668\uff1b\u5f53\u591a\u673a\u591aGPU\u65f6\uff0c\u8be5\u6807\u5fd7\u5728\u6bcf\u53f0\u673a\u5668\u7684\u4e3b\u8fdb\u7a0b\u4e2d\u90fd\u662fTrue\u3002\u5728\u591a\u673a\u591aGPU\u7684\u60c5\u51b5\u4e0b\u5982\u679c\u60f3\u8981\u53ea\u5728\u4e00\u53f0\u673a\u5668\u7684\u4e3b\u8fdb\u7a0b\u4e2d\u6267\u884c\uff0c\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\uff1a if accelerator.is_main_process: # Is executed once only \u6700\u540e\uff0c\u7531\u4e8e print \u51fd\u6570\u4f7f\u7528\u6bd4\u8f83\u9891\u7e41\uff0c\u6bcf\u6b21\u5305\u5230 if \u5206\u652f\u4e2d\u6bd4\u8f83\u9ebb\u70e6\uff0caccelerator\u5bf9\u5176\u5355\u72ec\u505a\u4e86\u5904\u7406\u3002\u76f4\u63a5\u4f7f\u7528 accelerator.print \u5373\u53ef\u5b9e\u73b0\u5728\u6bcf\u53f0\u673a\u5668\u4e2d\u53ea\u6253\u5370\u4e00\u6b21\u7684\u529f\u80fd\u3002 \u7b49\u5f85\u6240\u6709\u8fdb\u7a0b\u5b8c\u6210\uff0c\u518d\u7ee7\u7eed\u6267\u884c \u5728\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u4e0d\u53ef\u907f\u514d\u7684\u4f1a\u51fa\u73b0\u6709\u7684\u8fdb\u7a0b\u6267\u884c\u7684\u5feb\u3001\u6709\u7684\u8fdb\u7a0b\u6267\u884c\u7684\u6162\u7684\u60c5\u51b5\u3002\u800c\u6709\u4e9b\u65f6\u5019\u6211\u4eec\u5e0c\u671b\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5df2\u7ecf\u6267\u884c\u5b8c\u4e86\u5404\u81ea\u9700\u8981\u6267\u884c\u7684\u5185\u5bb9\u4e4b\u540e\uff0c\u518d\u8fdb\u884c\u540e\u9762\u7684\u3002\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u5b9e\u73b0\u8be5\u529f\u80fd\uff1a accelerator.wait_for_everyone() \u8fd9\u884c\u4ee3\u7801\u7684\u4f5c\u7528\u4e3a\uff1a\u5148\u6267\u884c\u5230\u8fd9\u884c\u4ee3\u7801\u7684\u8fdb\u7a0b\u963b\u585e\uff0c\u4e00\u76f4\u7b49\u5f85\u5230\u6240\u6709\u7684\u8fdb\u7a0b\u90fd\u6267\u884c\u5230\u4e86\u8fd9\u884c\u4ee3\u7801\u4e4b\u540e\uff0c\u518d\u540c\u65f6\u5f00\u59cb\u6267\u884c\u540e\u9762\u7684\u4ee3\u7801\uff1b \u5982\u679c\u662f\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u90a3\u4e48\u4e0a\u9762\u8fd9\u884c\u4ee3\u7801\u4ec0\u4e48\u90fd\u4e0d\u505a\u3002 \u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b Gradient clipping \u5982\u679c\u4e4b\u524d\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4e86 gradient clipping\uff0c\u90a3\u4e48\u9700\u8981: \u4f7f\u7528\u51fd\u6570 accelerator.clip_grad_norm_ \u66ff\u6362\u539f\u6765\u7684 torch.nn.utils.clip_grad_norm_ \uff1b \u4f7f\u7528\u51fd\u6570 accelerator.clip_grad_value_ \u66ff\u6362\u539f\u6765\u7684 torch.nn.utils.clip_grad_value_ \uff1b \u6df7\u5408\u7cbe\u5ea6 DeepSpeed \u5185\u90e8\u539f\u7406\u548c\u673a\u5236","title":"Quick tour"},{"location":"quick_tour/#quick-tour","text":"","title":"Quick tour"},{"location":"quick_tour/#_1","text":"\u5728\u4e4b\u524d\u7684\u6587\u6863 Accelerate \u4e2d\u5df2\u7ecf\u770b\u5230\u8fc7\u8981\u60f3\u4f7f\u7528Accelerate\u9700\u8981\u505a\u7684\u4ee3\u7801\u4fee\u6539\u5982\u4e0b\u6240\u793a\uff1a + from accelerate import Accelerator + accelerator = Accelerator() - my_model.to(device) # Pass every important object (model, optimizer, dataloader) to `accelerator.prepare` + my_model, my_optimizer, my_training_dataloader = accelerate.prepare( + my_model, my_optimizer, my_training_dataloader + ) for batch in my_training_dataloader: my_optimizer.zero_grad() inputs, targets = batch - inputs = inputs.to(device) - targets = targets.to(device) outputs = my_model(inputs) loss = my_loss_function(outputs, targets) # Just a small change for the backward instruction - loss.backward() + accelerate.backward(loss) my_optimizer.step() \u4e0a\u8ff0\u4ee3\u7801\u7684\u4fee\u6539\u53ef\u4ee5\u5206\u4e3a\u56db\u4e2a\u6b65\u9aa4\u8fdb\u884c\uff1a \u6b65\u9aa4\u4e00 \uff1aimport \u5e76\u5b9e\u4f8b\u5316 Accelerator \u5bf9\u8c61\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a from accelerate import Accelerator accelerator = Accelerator() \u4e0a\u8ff0\u8fd9\u4e2a\u521d\u59cb\u5316\u4ee3\u7801\u4f1a\u81ea\u52a8\u68c0\u6d4b\u8bbe\u5907\u548c\u73af\u5883\u4fe1\u606f\uff08\u5355CPU\u3001\u591aCPU\u3001\u5355GPU\u3001\u591aGPU\u3001TPU\u7b49\u7b49\u90fd\u4f1a\u81ea\u52a8\u68c0\u6d4b\u5230\uff09\uff0c\u5f00\u53d1\u8005\u5b8c\u5168\u4e0d\u9700\u8981\u5173\u5fc3\u8bbe\u5907\u548c\u73af\u5883\u4fe1\u606f\u3002 \u9700\u8981\u6ce8\u610f\u7684\u4e00\u70b9\u662f\uff1a\u4e0a\u8ff0\u521d\u59cb\u5316\u4ee3\u7801\u5fc5\u987b\u8981\u5728\u6240\u6709\u8bad\u7ec3\u6709\u5173\u7684\u4ee3\u7801\u4e4b\u524d\u6267\u884c\uff1b \u6b65\u9aa4\u4e8c \uff1a\u5220\u9664\u539f\u4ee3\u7801\u4e2d\u7684 to(device) \u548c cuda() \uff0cAccelerator \u4f1a\u81ea\u52a8\u5c06model\u3001data\u7b49\u653e\u5230\u6b63\u786e\u7684\u8bbe\u5907\u4e0a\uff0c\u5f00\u53d1\u8005\u4e0d\u7528\u5173\u5fc3\u3002 \u5f53\u7136\uff0c\u5982\u679c\u662f\u9ad8\u7aef\u73a9\u5bb6\u60f3\u8981\u81ea\u5df1\u6307\u5b9a\u8bbe\u5907\u4fe1\u606f\u4e5f\u53ef\u4ee5\uff0c\u6b64\u65f6 to(device) \u4e2d\u7684 device \u9700\u8981\u662f accelerator.device \u3002 \u6700\u540e\uff0c\u5982\u679c\u5b8c\u5168\u4e0d\u60f3\u4f7f\u7528 Accelerator \u81ea\u52a8\u7ba1\u7406\u8bbe\u5907\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u5728\u521d\u59cb\u5316\u65f6\u4f20\u5165\u53c2\u6570 device_placement=False \uff0c\u4ee3\u7801\u5982\u4e0b\uff1a accelerator = Accelerator(device_placement=False) \u6b65\u9aa4\u4e09 \uff1a\u5c06\u6240\u6709\u548c\u8bad\u7ec3\u76f8\u5173\u7684\u5bf9\u8c61\uff08 optimizer , model , training dataloader \uff09\u4f20\u5165\u51fd\u6570 prepare() \u3002\u8be5\u6b65\u9aa4\u5e94\u8be5\u653e\u5728\u6240\u6709\u76f8\u5173\u5bf9\u8c61\u521d\u59cb\u5316\u4e4b\u540e\uff0c\u548c\u8bad\u7ec3\u5f00\u59cb\u4e4b\u524d\u3002\u4ee3\u7801\u5982\u4e0b\uff1a model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader) \u6ce8\u610f\uff1a train_dataloader \uff1a\u5f53\u505a\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u4ee5\u591aGPU\u4e3a\u4f8b\uff0c\u6240\u6709\u7684\u8bad\u7ec3\u6570\u636e\u4f1a\u5e73\u5747\u5206\u914d\u5230\u6bcf\u4e2aGPU\u4e0a\u3002\u5f53\u8bad\u7ec3\u6570\u636e\u9700\u8981 shuffle \u65f6\uff0c\u5176\u5b9e\u73b0\u539f\u7406\u662f\uff1a \u5c06\u968f\u673a\u72b6\u6001\uff08random state\uff09\u540c\u6b65\u5230\u6240\u6709\u7684GPU\uff0c\u4ee5\u6b64\u4fdd\u8bc1\u6bcf\u4e2aGPU\u4e0a\u8bad\u7ec3\u6570\u636e shuffle \u4e4b\u540e\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff1b \u6bcf\u4e2aGPU\u8bfb\u53d6 shuffle \u540e\u7684\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u540c\u7247\u6bb5\uff1b\u6bd4\u5982\u67092\u4e2aGPU\u65f6\uff0c\u7b2c\u4e00\u4e2aGPU\u8bfb\u53d6\u7b2c [1, 3, 5, 7, ...] \u6761\u6570\u636e\uff0c\u7b2c\u4e8c\u4e2aGPU\u8bfb\u53d6\u7b2c [0, 2, 4, 6, ...] \u6761\u6570\u636e\uff1b batch_size \uff1a\u5f53\u505a\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u4ee5\u591aGPU\u4e3a\u4f8b\uff1a\u5b9e\u9645\u7684batch_size = \u8f93\u5165\u7684batch_size * GPU\u6570\u91cf\u3002\u6bd4\u5982\u8f93\u5165batch_size\u4e3a16\uff0c\u5f53\u4f7f\u75284\u4e2aGPU\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u5b9e\u9645\u7684batch_size\u4e3a64\u3002 validation dataloader \uff1a\u4e0a\u9762\u4f20\u5165 prepare() \u7684\u53ea\u6709 train_dataloader \uff0c\u53e6\u5916 validation dataloader \u4e5f\u53ef\u4ee5\u4f20\u5165 prepare() \uff0c\u5177\u4f53\u7684\u4f7f\u7528\u65b9\u5f0f\u89c1\u4e0b\u9762\u7684 \u5206\u5e03\u5f0f\u8bc4\u4f30 \u90e8\u5206\u3002 \u6b65\u9aa4\u56db \uff1a\u5c06 loss.backward() \u66ff\u6362\u6210 accelerator.backward(loss) \u3002 \u5b8c\u6210\u4ee5\u4e0a\u56db\u4e2a\u6b65\u9aa4\u4e4b\u540e\u5c31\u53ef\u4ee5\u6b63\u5e38\u7684\u4f7f\u7528 Accelerator \u4e86\u3002","title":"\u4e3b\u8981\u7528\u6cd5"},{"location":"quick_tour/#_2","text":"\u5982\u679c\u8981\u505a\u5206\u5e03\u5f0f\u8bc4\u4f30\uff0c\u53ea\u9700\u8981\u5c06 validation_dataloader \u4e5f\u4f20\u5165 prepare() \uff0c\u4ee3\u7801\u5982\u4e0b\uff1a validation_dataloader = accelerator.prepare(validation_dataloader) \u548c train_dataloader \u7684\u5b9e\u73b0\u539f\u7406\u7c7b\u4f3c\uff0c validation_dataloader \u4e2d\u6240\u6709\u7684\u6570\u636e\u4f1a\u88ab\u5212\u5206\u5230\u591a\u4e2aGPU\u4e0a\uff0c\u6240\u4ee5\u5728\u6a21\u578b\u524d\u5411\u4f20\u64ad\u5b8c\u6210\u4e4b\u540e\u8981\u8c03\u7528\u51fd\u6570 gather() \u505a\u4e00\u4e0b\u805a\u5408\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a for inputs, targets in validation_dataloader: predictions = model(inputs) # Gather all predictions and targets all_predictions = accelerator.gather(predictions) all_targets = accelerator.gather(targets) # Example of use with a `Datasets.Metric` metric.add_batch(all_predictions, all_targets) \u6ce8\u610f\uff1a\u51fd\u6570 gather() \u8981\u6c42\u6240\u6709\u7684 tensor \u5177\u6709\u76f8\u540c\u7684\u7ef4\u5ea6\u3002\u6bd4\u5982\uff1a\u6bcf\u4e2aGPU\u4e0a\u7684\u6570\u636e\u5728 padding \u65f6\uff0cpadding \u7684\u6700\u5927\u957f\u5ea6\u8bbe\u7f6e\u4e3a\u5f53\u524d\u8fd9\u4e2a mini_batch=16 \u6761\u6570\u636e\u4e2d\u957f\u5ea6\u6700\u5927\u7684\u90a3\u6761\u6570\u636e\u7684\u957f\u5ea6\uff0c\u5c31\u4f1a\u5bfc\u81f4\u591a\u4e2aGPU\u4e4b\u95f4\u7684 tensor \u7684\u7ef4\u5ea6\u4e0d\u540c\uff0c\u6b64\u65f6\u51fd\u6570 gather() \u4e0d\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002","title":"\u5206\u5e03\u5f0f\u8bc4\u4f30"},{"location":"quick_tour/#_3","text":"\u6ca1\u6709\u770b","title":"\u8fd0\u884c\u5206\u5e03\u5f0f\u811a\u672c"},{"location":"quick_tour/#notebook","text":"\u6ca1\u6709\u770b","title":"\u4f7f\u7528notebook\u8fd0\u884c"},{"location":"quick_tour/#tpu","text":"\u6211\u54ea\u6709TPU","title":"\u5728TPU\u4e0a\u8bad\u7ec3"},{"location":"quick_tour/#_4","text":"","title":"\u5176\u4ed6\u6ce8\u610f\u70b9\u8bf4\u660e"},{"location":"quick_tour/#_5","text":"\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u4ee5\u591aGPU\u4e3a\u4f8b\uff0c\u6211\u4eec\u7684\u7a0b\u5e8f\u4f1a\u540c\u65f6\u8fd0\u884c\u5728\u591a\u5f20\u663e\u5361\u4e0a\u3002\u90a3\u4e48\u6bd4\u5982\u6253\u5370\u65e5\u5fd7\u3001\u6253\u5370\u8fdb\u5ea6\u6761\u7b49\u529f\u80fd\uff0c\u53ea\u5e0c\u671b\u5176\u6267\u884c\u4e00\u6b21\u5c31\u884c\u4e86\uff0c\u5426\u5219\u5982\u679c\u67098\u5f20\u663e\u5361\uff0c\u540c\u4e00\u6761\u65e5\u5fd7\u6253\u53708\u6b21\u770b\u8d77\u6765\u5f88\u4e0d\u65b9\u4fbf\u3002 \u60f3\u8981\u5b9e\u73b0\u5e76\u53d1\u8bad\u7ec3\u65f6\u7a0b\u5e8f\u53ea\u6267\u884c\u4e00\u6b21\uff0c\u53ea\u9700\u5c06\u7a0b\u5e8f\u5305\u5230\u5982\u4e0b if \u5224\u65ad\u5185\u90e8\u5373\u53ef\uff1a if accelerator.is_local_main_process: # Is executed once per server \u5176\u4e2d\u6807\u5fd7 accelerator.is_local_main_process \u5c31\u8868\u793a\u662f\u5426\u5728\u4e3b\u8fdb\u7a0b\u4e2d\u3002\u5728\u8fd9\u4e2a if \u5224\u65ad\u5185\u90e8\u7684\u4ee3\u7801\uff0c\u5982\u679c\u4e0d\u662f\u5728\u4e3b\u8fdb\u7a0b\u4e2d\uff0c\u662f\u4e0d\u4f1a\u6267\u884c\u7684\u3002 \u50cf\u6253\u5370\u8fdb\u5ea6\u6761\u7684 tqdm \u5e93\uff0c\u53ef\u4ee5\u6309\u7167\u5982\u4e0b\u65b9\u5f0f\u4f7f\u7528\uff1a from tqdm.auto import tqdm progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process) \u7c7b\u4f3c\u7684\uff0c\u53ea\u6709\u5728\u4e3b\u8fdb\u7a0b\u4e2d\u624d\u4f1a\u6253\u5370\u8fdb\u5ea6\u6761\uff1b \u6807\u5fd7 accelerator.is_local_main_process \u4e2d\u7684 local \u662f\u5f53\u524d\u673a\u5668\uff1b\u5f53\u591a\u673a\u591aGPU\u65f6\uff0c\u8be5\u6807\u5fd7\u5728\u6bcf\u53f0\u673a\u5668\u7684\u4e3b\u8fdb\u7a0b\u4e2d\u90fd\u662fTrue\u3002\u5728\u591a\u673a\u591aGPU\u7684\u60c5\u51b5\u4e0b\u5982\u679c\u60f3\u8981\u53ea\u5728\u4e00\u53f0\u673a\u5668\u7684\u4e3b\u8fdb\u7a0b\u4e2d\u6267\u884c\uff0c\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\uff1a if accelerator.is_main_process: # Is executed once only \u6700\u540e\uff0c\u7531\u4e8e print \u51fd\u6570\u4f7f\u7528\u6bd4\u8f83\u9891\u7e41\uff0c\u6bcf\u6b21\u5305\u5230 if \u5206\u652f\u4e2d\u6bd4\u8f83\u9ebb\u70e6\uff0caccelerator\u5bf9\u5176\u5355\u72ec\u505a\u4e86\u5904\u7406\u3002\u76f4\u63a5\u4f7f\u7528 accelerator.print \u5373\u53ef\u5b9e\u73b0\u5728\u6bcf\u53f0\u673a\u5668\u4e2d\u53ea\u6253\u5370\u4e00\u6b21\u7684\u529f\u80fd\u3002","title":"\u7a0b\u5e8f\u53ea\u5728\u4e00\u4e2a\u8fdb\u7a0b\u4e2d\u6267\u884c"},{"location":"quick_tour/#_6","text":"\u5728\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u4e0d\u53ef\u907f\u514d\u7684\u4f1a\u51fa\u73b0\u6709\u7684\u8fdb\u7a0b\u6267\u884c\u7684\u5feb\u3001\u6709\u7684\u8fdb\u7a0b\u6267\u884c\u7684\u6162\u7684\u60c5\u51b5\u3002\u800c\u6709\u4e9b\u65f6\u5019\u6211\u4eec\u5e0c\u671b\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5df2\u7ecf\u6267\u884c\u5b8c\u4e86\u5404\u81ea\u9700\u8981\u6267\u884c\u7684\u5185\u5bb9\u4e4b\u540e\uff0c\u518d\u8fdb\u884c\u540e\u9762\u7684\u3002\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u5b9e\u73b0\u8be5\u529f\u80fd\uff1a accelerator.wait_for_everyone() \u8fd9\u884c\u4ee3\u7801\u7684\u4f5c\u7528\u4e3a\uff1a\u5148\u6267\u884c\u5230\u8fd9\u884c\u4ee3\u7801\u7684\u8fdb\u7a0b\u963b\u585e\uff0c\u4e00\u76f4\u7b49\u5f85\u5230\u6240\u6709\u7684\u8fdb\u7a0b\u90fd\u6267\u884c\u5230\u4e86\u8fd9\u884c\u4ee3\u7801\u4e4b\u540e\uff0c\u518d\u540c\u65f6\u5f00\u59cb\u6267\u884c\u540e\u9762\u7684\u4ee3\u7801\uff1b \u5982\u679c\u662f\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u90a3\u4e48\u4e0a\u9762\u8fd9\u884c\u4ee3\u7801\u4ec0\u4e48\u90fd\u4e0d\u505a\u3002","title":"\u7b49\u5f85\u6240\u6709\u8fdb\u7a0b\u5b8c\u6210\uff0c\u518d\u7ee7\u7eed\u6267\u884c"},{"location":"quick_tour/#_7","text":"","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b"},{"location":"quick_tour/#gradient-clipping","text":"\u5982\u679c\u4e4b\u524d\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4e86 gradient clipping\uff0c\u90a3\u4e48\u9700\u8981: \u4f7f\u7528\u51fd\u6570 accelerator.clip_grad_norm_ \u66ff\u6362\u539f\u6765\u7684 torch.nn.utils.clip_grad_norm_ \uff1b \u4f7f\u7528\u51fd\u6570 accelerator.clip_grad_value_ \u66ff\u6362\u539f\u6765\u7684 torch.nn.utils.clip_grad_value_ \uff1b","title":"Gradient clipping"},{"location":"quick_tour/#_8","text":"","title":"\u6df7\u5408\u7cbe\u5ea6"},{"location":"quick_tour/#deepspeed","text":"","title":"DeepSpeed"},{"location":"quick_tour/#_9","text":"","title":"\u5185\u90e8\u539f\u7406\u548c\u673a\u5236"}]}